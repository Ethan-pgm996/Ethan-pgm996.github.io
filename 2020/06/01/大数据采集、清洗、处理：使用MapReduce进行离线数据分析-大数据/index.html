<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>大数据采集、清洗、处理：使用MapReduce进行离线数据分析---大数据 | 艾利克斯工作室</title><meta name="description" content="在互联网应用中，不管是哪一种处理方式，其基本的数据来源都是日志数据，例如对于web应用来说，则可能是用户的访问日志、用户的点击日志等。如果对于数据的分析结果在时间上有比较严格的要求，则可以采用在线处理的方式来对数据进行分析，如使用Spark、Storm等进行处理。比较贴切的一个例子是天猫双十一的成交额，在其展板上，我们看到交易额是实时动态进行更新的，对于这种情况，则需要采用在线处理。当然，如果只是"><meta name="keywords" content="Java大数据开发"><meta name="author" content="Alexis (KangJ)"><meta name="copyright" content="Alexis (KangJ)"><meta name="format-detection" content="telephone=no"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://programmer996.club/2020/06/01/%E5%A4%A7%E6%95%B0%E6%8D%AE%E9%87%87%E9%9B%86%E3%80%81%E6%B8%85%E6%B4%97%E3%80%81%E5%A4%84%E7%90%86%EF%BC%9A%E4%BD%BF%E7%94%A8MapReduce%E8%BF%9B%E8%A1%8C%E7%A6%BB%E7%BA%BF%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90-%E5%A4%A7%E6%95%B0%E6%8D%AE/"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//fonts.googleapis.com" crossorigin="crossorigin"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><meta property="og:type" content="article"><meta property="og:title" content="大数据采集、清洗、处理：使用MapReduce进行离线数据分析---大数据"><meta property="og:url" content="https://programmer996.club/2020/06/01/%E5%A4%A7%E6%95%B0%E6%8D%AE%E9%87%87%E9%9B%86%E3%80%81%E6%B8%85%E6%B4%97%E3%80%81%E5%A4%84%E7%90%86%EF%BC%9A%E4%BD%BF%E7%94%A8MapReduce%E8%BF%9B%E8%A1%8C%E7%A6%BB%E7%BA%BF%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90-%E5%A4%A7%E6%95%B0%E6%8D%AE/"><meta property="og:site_name" content="艾利克斯工作室"><meta property="og:description" content="在互联网应用中，不管是哪一种处理方式，其基本的数据来源都是日志数据，例如对于web应用来说，则可能是用户的访问日志、用户的点击日志等。如果对于数据的分析结果在时间上有比较严格的要求，则可以采用在线处理的方式来对数据进行分析，如使用Spark、Storm等进行处理。比较贴切的一个例子是天猫双十一的成交额，在其展板上，我们看到交易额是实时动态进行更新的，对于这种情况，则需要采用在线处理。当然，如果只是"><meta property="og:image" content="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg"><meta property="article:published_time" content="2020-06-01T14:38:56.000Z"><meta property="article:modified_time" content="2020-06-01T15:12:20.000Z"><meta name="twitter:card" content="summary"><script>var activateDarkMode = function () {
  document.documentElement.setAttribute('data-theme', 'dark')
  if (document.querySelector('meta[name="theme-color"]') !== null) {
    document.querySelector('meta[name="theme-color"]').setAttribute('content', '#000')
  }
}
var activateLightMode = function () {
  document.documentElement.setAttribute('data-theme', 'light')
  if (document.querySelector('meta[name="theme-color"]') !== null) {
    document.querySelector('meta[name="theme-color"]').setAttribute('content', '#fff')
  }
}

var getCookies = function (name) {
  const value = `; ${document.cookie}`
  const parts = value.split(`; ${name}=`)
  if (parts.length === 2) return parts.pop().split(';').shift()
}

var autoChangeMode = 'false'
var t = getCookies('theme')
if (autoChangeMode === '1') {
  var isDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches
  var isLightMode = window.matchMedia('(prefers-color-scheme: light)').matches
  var isNotSpecified = window.matchMedia('(prefers-color-scheme: no-preference)').matches
  var hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

  if (t === undefined) {
    if (isLightMode) activateLightMode()
    else if (isDarkMode) activateDarkMode()
    else if (isNotSpecified || hasNoSupport) {
      console.log('You specified no preference for a color scheme or your browser does not support it. I Schedule dark mode during night time.')
      var now = new Date()
      var hour = now.getHours()
      var isNight = hour <= 6 || hour >= 18
      isNight ? activateDarkMode() : activateLightMode()
    }
    window.matchMedia('(prefers-color-scheme: dark)').addListener(function (e) {
      if (Cookies.get('theme') === undefined) {
        e.matches ? activateDarkMode() : activateLightMode()
      }
    })
  } else if (t === 'light') activateLightMode()
  else activateDarkMode()
} else if (autoChangeMode === '2') {
  now = new Date()
  hour = now.getHours()
  isNight = hour <= 6 || hour >= 18
  if (t === undefined) isNight ? activateDarkMode() : activateLightMode()
  else if (t === 'light') activateLightMode()
  else activateDarkMode()
} else {
  if (t === 'dark') activateDarkMode()
  else if (t === 'light') activateLightMode()
}</script><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.css"><link rel="prev" title="PyTorch专栏开篇" href="https://programmer996.club/2020/07/02/Pytorch%E4%B8%93%E6%A0%8F%E5%BC%80%E7%AF%87/"><link rel="next" title="Spark生态圈---大数据" href="https://programmer996.club/2020/06/01/Spark%E7%94%9F%E6%80%81%E5%9C%88-%E5%A4%A7%E6%95%B0%E6%8D%AE/"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web&amp;display=swap"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  noticeOutdate: undefined,
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  bookmark: {
    message_prev: 'Press',
    message_next: 'to bookmark this page'
  },
  runtime_unit: 'days',
  runtime: false,
  copyright: undefined,
  ClickShowText: undefined,
  medium_zoom: false,
  fancybox: true,
  Snackbar: {"bookmark":{"message_prev":"Press","message_next":"to bookmark this page"},"chs_to_cht":"Traditional Chinese Activated Manually","cht_to_chs":"Simplified Chinese Activated Manually","day_to_night":"Dark Mode Activated Manually","night_to_day":"Light Mode Activated Manually","bgLight":"#49b1f5","bgDark":"#121212","position":"bottom-left"},
  justifiedGallery: {
    js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
    css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
  },
  baiduPush: false,
  highlightCopy: true,
  highlightLang: true,
  isPhotoFigcaption: false,
  islazyload: true,
  isanchor: false    
}</script><script>var GLOBAL_CONFIG_SITE = { 
  isPost: true,
  isHome: false,
  isHighlightShrink: true,
  isSidebar: true,
  postUpdate: '2020-06-01 23:12:20'
}</script><noscript><style>
#nav {
  opacity: 1
}
.justified-gallery img{
  opacity: 1
}
</style></noscript><meta name="generator" content="Hexo 4.2.1"><link rel="alternate" href="/atom.xml" title="艾利克斯工作室" type="application/atom+xml">
</head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">Loading...</div></div></div><div id="mobile-sidebar"><div id="menu_mask"></div><div id="mobile-sidebar-menus"><div class="mobile_author_icon"><img class="avatar-img" src="/img/avatar.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="mobile_post_data"><div class="mobile_data_item is-center"><div class="mobile_data_link"><a href="/archives/"><div class="headline">Articles</div><div class="length_num">45</div></a></div></div><div class="mobile_data_item is-center">      <div class="mobile_data_link"><a href="/tags/"><div class="headline">Tags</div><div class="length_num">9</div></a></div></div><div class="mobile_data_item is-center">     <div class="mobile_data_link"><a href="/categories/"><div class="headline">Categories</div><div class="length_num">16</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/book/"><i class="fa-fw fas fa-gift"></i><span> book</span></a></div><div class="menus_item"><a class="site-page"><i class="fa-fw fas fa-list"></i><span> List</span><i class="fas fa-chevron-down menus-expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/welfare/"><i class="fa-fw fas fa-gift"></i><span> Welfare</span></a></li><li><a class="site-page" href="/Case/"><i class="fa-fw fas fa-briefcase"></i><span> Case</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div><div class="menus_item"><a class="site-page" href="/Cooperation/"><i class="fa-fw fas fa-briefcase"></i><span> Cooperation</span></a></div><div class="menus_item"><a class="site-page" href="/handshake/"><i class="fa-fw fas fa-eye"></i><span> handshake</span></a></div></div></div></div><i class="fas fa-arrow-right on" id="toggle-sidebar"></i><div id="sidebar"><div class="sidebar-toc"><div class="sidebar-toc__title">Catalog</div><div class="sidebar-toc__progress"><span class="progress-notice">You've read</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar">     </div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#大数据处理的常用方法"><span class="toc-number">1.</span> <span class="toc-text">大数据处理的常用方法</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#生产场景与需求"><span class="toc-number">2.</span> <span class="toc-text">生产场景与需求</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#数据采集：获取原生数据"><span class="toc-number">3.</span> <span class="toc-text">数据采集：获取原生数据</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#数据清洗："><span class="toc-number">4.</span> <span class="toc-text">数据清洗：</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#数据清洗目的"><span class="toc-number">5.</span> <span class="toc-text">数据清洗目的</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#数据清洗方案"><span class="toc-number">6.</span> <span class="toc-text">数据清洗方案</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#数据清洗过程："><span class="toc-number">7.</span> <span class="toc-text">数据清洗过程：</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#执行MapReduce程序"><span class="toc-number">8.</span> <span class="toc-text">执行MapReduce程序</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#数据清洗结果"><span class="toc-number">9.</span> <span class="toc-text">数据清洗结果</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#数据处理："><span class="toc-number">10.</span> <span class="toc-text">数据处理：</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#数据处理思路："><span class="toc-number">11.</span> <span class="toc-text">数据处理思路：</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#执行MapReduce程序-1"><span class="toc-number">12.</span> <span class="toc-text">执行MapReduce程序</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#数据处理结果"><span class="toc-number">13.</span> <span class="toc-text">数据处理结果</span></a></li></ol></div></div></div><div class="code-close" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg)"><nav id="nav"><span class="pull-left" id="blog_name"><a class="blog_title" id="site-name" href="/">艾利克斯工作室</a></span><span class="pull-right menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/book/"><i class="fa-fw fas fa-gift"></i><span> book</span></a></div><div class="menus_item"><a class="site-page"><i class="fa-fw fas fa-list"></i><span> List</span><i class="fas fa-chevron-down menus-expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/welfare/"><i class="fa-fw fas fa-gift"></i><span> Welfare</span></a></li><li><a class="site-page" href="/Case/"><i class="fa-fw fas fa-briefcase"></i><span> Case</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div><div class="menus_item"><a class="site-page" href="/Cooperation/"><i class="fa-fw fas fa-briefcase"></i><span> Cooperation</span></a></div><div class="menus_item"><a class="site-page" href="/handshake/"><i class="fa-fw fas fa-eye"></i><span> handshake</span></a></div></div><span class="toggle-menu close"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></span></span></nav><div id="post-info"><div id="post-title"><div class="posttitle">大数据采集、清洗、处理：使用MapReduce进行离线数据分析---大数据</div></div><div id="post-meta"><div class="meta-firstline"><time class="post-meta__date"><span class="post-meta__date-created" title="Created 2020-06-01 22:38:56"><i class="far fa-calendar-alt fa-fw"></i> Created 2020-06-01</span><span class="post-meta__separator">|</span><span class="post-meta__date-updated" title="Updated 2020-06-01 23:12:20"><i class="fas fa-history fa-fw"></i> Updated 2020-06-01</span></time><span class="post-meta__categories"><span class="post-meta__separator">|</span><i class="fas fa-inbox fa-fw post-meta__icon"></i><a class="post-meta__categories" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/">大数据</a></span></div><div class="meta-secondline"> <span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta__icon"></i><span>Word count:</span><span class="word-count">4.6k</span><span class="post-meta__separator">|</span><i class="far fa-clock fa-fw post-meta__icon"></i><span>Reading time: 19 min</span></span></div><div class="meta-thirdline"><span class="post-meta-pv-cv"><span class="post-meta__separator">|</span><i class="far fa-eye fa-fw post-meta__icon"></i><span>Post View:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout_post" id="content-inner"><article id="post"><div class="post-content" id="article-container"><p>在互联网应用中，不管是哪一种处理方式，其基本的数据来源都是日志数据<a id="more"></a>，例如对于web应用来说，则可能是用户的访问日志、用户的点击日志等。<br>如果对于数据的分析结果在时间上有比较严格的要求，则可以采用在线处理的方式来对数据进行分析，如使用Spark、Storm等进行处理。比较贴切的一个例子是天猫双十一的成交额，在其展板上，我们看到交易额是实时动态进行更新的，对于这种情况，则需要采用在线处理。<br>当然，如果只是希望得到数据的分析结果，对处理的时间要求不严格，就可以采用离线处理的方式，比如我们可以先将日志数据采集到HDFS中，之后再进一步使用MapReduce、Hive等来对数据进行分析，这也是可行的。<br>这份文档主要分享对某个电商网站产生的用户访问日志（access.log）进行离线处理与分析的过程，基于MapReduce的处理方式，最后会统计出某一天不同省份访问该网站的uv与pv。</p>
<h1 id="大数据处理的常用方法"><a href="#大数据处理的常用方法" class="headerlink" title="大数据处理的常用方法"></a>大数据处理的常用方法</h1><p>大数据处理目前比较流行的是两种方法，一种是离线处理，一种是在线处理。</p>
<h1 id="生产场景与需求"><a href="#生产场景与需求" class="headerlink" title="生产场景与需求"></a>生产场景与需求</h1><p>在我们的场景中，Web应用的部署是如下的架构：<br>即比较典型的Nginx负载均衡+KeepAlive高可用集群架构，在每台Web服务器上，都会产生用户的访问日志，业务需求方给出的日志格式如下：  </p>
<pre><code>1001    211.167.248.22  eecf0780-2578-4d77-a8d6-e2225e8b9169    40604   1       GET /top HTTP/1.0       408     null      null    1523188122767
1003    222.68.207.11   eecf0780-2578-4d77-a8d6-e2225e8b9169    20202   1       GET /tologin HTTP/1.1   504     null      Mozilla/5.0 (Windows; U; Windows NT 5.1)Gecko/20070309 Firefox/2.0.0.3  1523188123267
1001    61.53.137.50    c3966af9-8a43-4bda-b58c-c11525ca367b    0       1       GET /update/pass HTTP/1.0       302       null    null    1523188123768
1000    221.195.40.145  1aa3b538-2f55-4cd7-9f46-6364fdd1e487    0       0       GET /user/add HTTP/1.1  200     null      Mozilla/4.0 (compatible; MSIE 7.0; Windows NT5.2)       1523188124269
1000    121.11.87.171   8b0ea90a-77a5-4034-99ed-403c800263dd    20202   1       GET /top HTTP/1.0       408     null      Mozilla/5.0 (Windows; U; Windows NT 5.1)Gecko/20070803 Firefox/1.5.0.12 1523188120263</code></pre><p>其每个字段的说明如下：  </p>
<pre><code>appid ip mid userid login_type request status http_referer user_agent time
其中：
appid包括：web:1000,android:1001,ios:1002,ipad:1003
mid:唯一的id此id第一次会种在浏览器的cookie里。如果存在则不再种。作为浏览器唯一标示。移动端或者pad直接取机器码。
login_type：登录状态，0未登录、1：登录用户
request：类似于此种 &quot;GET /userList HTTP/1.1&quot;
status：请求的状态主要有：200 ok、404 not found、408 Request Timeout、500 Internal Server Error、504 Gateway Timeout等
http_referer：请求该url的上一个url地址。
user_agent：浏览器的信息，例如：&quot;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/47.0.2526.106 Safari/537.36&quot;
time：时间的long格式：1451451433818。</code></pre><p>根据给定的时间范围内的日志数据，现在业务方有如下需求：<br>统计出每个省每日访问的PV、UV。  </p>
<h1 id="数据采集：获取原生数据"><a href="#数据采集：获取原生数据" class="headerlink" title="数据采集：获取原生数据"></a>数据采集：获取原生数据</h1><p>对于用户访问日志的采集，使用的是Flume，并且会将采集的数据保存到HDFS中，其架构思路如下：  </p>
<pre><code>不同的Web Server上都会部署一个Agent用于该Server上日志数据的采集，之后，不同Web Server的Flume Agent采集的日志数据会下沉到另外一个被称为Flume Consolidation Agent（聚合Agent）的Flume Agent上，该Flume Agent的数据落地方式为输出到HDFS。</code></pre><p>在我们的HDFS中，可以查看到其采集的日志：<br><img src= "/img/loading.gif" data-src="/images/hadoop.png"></p>
<p>后面我们的工作正是要基于Flume采集到HDFS中的数据做离线处理与分析。</p>
<h1 id="数据清洗："><a href="#数据清洗：" class="headerlink" title="数据清洗："></a>数据清洗：</h1><p>将不规整数据转化为规整数据</p>
<h1 id="数据清洗目的"><a href="#数据清洗目的" class="headerlink" title="数据清洗目的"></a>数据清洗目的</h1><p>刚刚采集到HDFS中的原生数据，我们也称为不规整数据，即目前来说，该数据的格式还无法满足我们对数据处理的基本要求，需要对其进行预处理，转化为我们后面工作所需要的较为规整的数据，所以这里的数据清洗，其实指的就是对数据进行基本的预处理，以方便我们后面的统计分析，所以这一步并不是必须的，需要根据不同的业务需求来进行取舍，只是在我们的场景中需要对数据进行一定的处理。</p>
<h1 id="数据清洗方案"><a href="#数据清洗方案" class="headerlink" title="数据清洗方案"></a>数据清洗方案</h1><p>原来的日志数据格式是如下的：</p>
<pre><code>appid ip mid userid login_type request status http_referer user_agent time
其中：
appid包括：web:1000,android:1001,ios:1002,ipad:1003
mid:唯一的id此id第一次会种在浏览器的cookie里。如果存在则不再种。作为浏览器唯一标示。移动端或者pad直接取机器码。
login_type：登录状态，0未登录、1：登录用户
request：类似于此种 &quot;GET /userList HTTP/1.1&quot;
status：请求的状态主要有：200 ok、404 not found、408 Request Timeout、500 Internal Server Error、504 Gateway Timeout等
http_referer：请求该url的上一个url地址。
user_agent：浏览器的信息，例如：&quot;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/47.0.2526.106 Safari/537.36&quot;
time：时间的long格式：1451451433818。</code></pre><p>但是如果需要按照省份来统计uv、pv，其所包含的信息还不够，我们需要对这些数据做一定的预处理，比如需要，对于其中包含的IP信息，我们需要将其对应的IP信息解析出来；为了方便我们的其它统计，我们也可以将其request信息解析为method、 request_url、 http_version等，所以按照上面的分析，我们希望预处理之后的日志数据包含如下的数据字段：  </p>
<pre><code>appid;  
ip;
//通过ip来衍生出来的字段 province和city
province;
city;

mid;      
userId;    
loginType; 
request; 
//通过request 衍生出来的字段 method request_url http_version
method;
requestUrl;
httpVersion;

status;          
httpReferer; 
userAgent;   
//通过userAgent衍生出来的字段，即用户的浏览器信息
browser;

time;</code></pre><p>即在原来的基础上，我们增加了其它新的字段，如province、city等。  </p>
<p>我们采用MapReduce来对数据进行预处理，预处理之后的结果，我们也是保存到HDFS中，即采用如下的架构：<br><img src= "/img/loading.gif" data-src="/images/hadoop5.png"></p>
<h1 id="数据清洗过程："><a href="#数据清洗过程：" class="headerlink" title="数据清洗过程："></a>数据清洗过程：</h1><p>MapReduce程序编写<br>数据清洗的过程主要是编写MapReduce程序，而MapReduce程序的编写又分为写Mapper、Reducer、Job三个基本的过程。但是在我们这个案例中，要达到数据清洗的目的，实际上只需要Mapper就可以了，并不需要Reducer，原因很简单，我们只是预处理数据，在Mapper中就已经可以对数据进行处理了，其输出的数据并不需要进一步经过Redcuer来进行汇总处理。  </p>
<p>所以下面就直接编写Mapper和Job的程序代码:<br>AccessLogCleanMapper</p>
<pre><code>package cn.xpleaf.dataClean.mr.mapper;

import cn.xpleaf.dataClean.mr.writable.AccessLogWritable;
import cn.xpleaf.dataClean.utils.JedisUtil;
import cn.xpleaf.dataClean.utils.UserAgent;
import cn.xpleaf.dataClean.utils.UserAgentUtil;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.NullWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.log4j.Logger;
import redis.clients.jedis.Jedis;

import java.io.IOException;
import java.text.DateFormat;
import java.text.SimpleDateFormat;
import java.util.Date;

/**
 * access日志清洗的主要mapper实现类
 * 原始数据结构：
 * appid ip mid userid login_tpe request status http_referer user_agent time ---&gt; 10列内容
 * 清洗之后的结果：
 * appid ip province city mid userid login_type request method request_url http_version status http_referer user_agent browser yyyy-MM-dd HH:mm:ss
 */
public class AccessLogCleanMapper extends Mapper&lt;LongWritable, Text, NullWritable, Text&gt; {

    private Logger logger;
    private String[] fields;

    private String appid;      //数据来源 web:1000,android:1001,ios:1002,ipad:1003
    private String ip;
    //通过ip来衍生出来的字段 province和city
    private String province;
    private String city;

    private String mid;      //mid:唯一的id此id第一次会种在浏览器的cookie里。如果存在则不再种。作为浏览器唯一标示。移动端或者pad直接取机器码。
    private String userId;     //用户id
    private String loginType; //登录状态，0未登录、1：登录用户
    private String request; //类似于此种 &quot;GET userList HTTP/1.1&quot;
    //通过request 衍生出来的字段 method request_url http_version
    private String method;
    private String requestUrl;
    private String httpVersion;

    private String status;          //请求的状态主要有：200 ok、/404 not found、408 Request Timeout、500 Internal Server Error、504 Gateway Timeout等
    private String httpReferer; //请求该url的上一个url地址。
    private String userAgent;   //浏览器的信息，例如：&quot;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/47.0.2526.106 Safari/537.36&quot;
    //通过userAgent来获取对应的浏览器
    private String browser;

    //private long time; //action对应的时间戳
    private String time;//action对应的格式化时间yyyy-MM-dd HH:mm:ss

    private DateFormat df;
    private Jedis jedis;

    @Override
    protected void setup(Context context) throws IOException, InterruptedException {
        logger = Logger.getLogger(AccessLogCleanMapper.class);
        df = new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;);
        jedis = JedisUtil.getJedis();
    }

    /**
     * appid ip mid userid login_tpe request status http_referer user_agent time ---&gt; 10列内容
     * ||
     * ||
     * appid ip province city mid userid login_type request method request_url http_version status http_referer user_agent browser yyyy-MM-dd HH:mm:ss
     */
    @Override
    protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
        fields = value.toString().split(&quot;\t&quot;);
        if (fields == null || fields.length != 10) { // 有异常数据
            return;
        }
        // 因为所有的字段没有进行特殊操作，只是文本的输出，所以没有必要设置特定类型，全部设置为字符串即可，
        // 这样在做下面的操作时就可以省去类型的转换，但是如果对数据的合法性有严格的验证的话，则要保持类型的一致
        appid = fields[0];
        ip = fields[1];
        // 解析IP
        if (ip != null) {
            String ipInfo = jedis.hget(&quot;ip_info&quot;, ip);
            province = ipInfo.split(&quot;\t&quot;)[0];
            city = ipInfo.split(&quot;\t&quot;)[1];
        }

        mid = fields[2];
        userId = fields[3];
        loginType = fields[4];
        request = fields[5];
        method = request.split(&quot; &quot;)[0];
        requestUrl = request.split(&quot; &quot;)[1];
        httpVersion = request.split(&quot; &quot;)[2];

        status = fields[6];
        httpReferer = fields[7];
        userAgent = fields[8];
        if (userAgent != null) {
            UserAgent uAgent = UserAgentUtil.getUserAgent(userAgent);
            if (uAgent != null) {
                browser = uAgent.getBrowserType();
            }
        }
        try { // 转换有可能出现异常
            time = df.format(new Date(Long.parseLong(fields[9])));
        } catch (NumberFormatException e) {
            logger.error(e.getMessage());
        }
        AccessLogWritable access = new AccessLogWritable(appid, ip, province, city, mid,
                userId, loginType, request, method, requestUrl,
                httpVersion, status, httpReferer, this.userAgent, browser, time);
        context.write(NullWritable.get(), new Text(access.toString()));
    }

    @Override
    protected void cleanup(Context context) throws IOException, InterruptedException {
        // 资源释放
        logger = null;
        df = null;
        JedisUtil.returnJedis(jedis);
    }
}</code></pre><p>AccessLogCleanJob  </p>
<pre><code>package cn.xpleaf.dataClean.mr.job;

import cn.xpleaf.dataClean.mr.mapper.AccessLogCleanMapper;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.NullWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;

/**
 * 清洗用户access日志信息
 * 主要的驱动程序
 *      主要用作组织mapper和reducer的运行
 *
 * 输入参数：
 * hdfs://ns1/input/data-clean/access/2018/04/08 hdfs://ns1/output/data-clean/access
 * 即inputPath和outputPath
 * 目前outputPath统一到hdfs://ns1/output/data-clean/access
 * 而inputPath则不确定，因为我们的日志采集是按天来生成一个目录的
 * 所以上面的inputPath只是清洗2018-04-08这一天的
 */
public class AccessLogCleanJob {
    public static void main(String[] args) throws Exception {

        if(args == null || args.length &lt; 2) {
            System.err.println(&quot;Parameter Errors! Usage &lt;inputPath...&gt; &lt;outputPath&gt;&quot;);
            System.exit(-1);
        }

        Path outputPath = new Path(args[args.length - 1]);

        Configuration conf = new Configuration();
        String jobName = AccessLogCleanJob.class.getSimpleName();
        Job job = Job.getInstance(conf, jobName);
        job.setJarByClass(AccessLogCleanJob.class);

        // 设置mr的输入参数
        for( int i = 0; i &lt; args.length - 1; i++) {
            FileInputFormat.addInputPath(job, new Path(args[i]));
        }
        job.setInputFormatClass(TextInputFormat.class);
        job.setMapperClass(AccessLogCleanMapper.class);
        job.setMapOutputKeyClass(NullWritable.class);
        job.setMapOutputValueClass(Text.class);
        // 设置mr的输出参数
        outputPath.getFileSystem(conf).delete(outputPath, true);    // 避免job在运行的时候出现输出目录已经存在的异常
        FileOutputFormat.setOutputPath(job, outputPath);
        job.setOutputFormatClass(TextOutputFormat.class);

        job.setOutputKeyClass(NullWritable.class);
        job.setOutputValueClass(Text.class);
        job.setNumReduceTasks(0);   // map only操作，没有reducer

        job.waitForCompletion(true);
    }
}</code></pre><h1 id="执行MapReduce程序"><a href="#执行MapReduce程序" class="headerlink" title="执行MapReduce程序"></a>执行MapReduce程序</h1><p>将上面的mr程序打包后上传到我们的Hadoop环境中，这里，对2018-04-08这一天产生的日志数据进行清洗，执行如下命令：  </p>
<pre><code>yarn jar data-extract-clean-analysis-1.0-SNAPSHOT-jar-with-dependencies.jar\
cn.xpleaf.dataClean.mr.job.AccessLogCleanJob \
hdfs://ns1/input/data-clean/access/2018/04/08 \
hdfs://ns1/output/data-clean/access</code></pre><p>观察结果，可以看到MapReduce Job执行成功！</p>
<pre><code>......
18/04/08 20:54:21 INFO mapreduce.Job: Running job: job_1523133033819_0009
18/04/08 20:54:28 INFO mapreduce.Job: Job job_1523133033819_0009 running in uber mode : false
18/04/08 20:54:28 INFO mapreduce.Job:  map 0% reduce 0%
18/04/08 20:54:35 INFO mapreduce.Job:  map 50% reduce 0%
18/04/08 20:54:40 INFO mapreduce.Job:  map 76% reduce 0%
18/04/08 20:54:43 INFO mapreduce.Job:  map 92% reduce 0%
18/04/08 20:54:45 INFO mapreduce.Job:  map 100% reduce 0%
18/04/08 20:54:46 INFO mapreduce.Job: Job job_1523133033819_0009 completed successfully
18/04/08 20:54:46 INFO mapreduce.Job: Counters: 31
......</code></pre><h1 id="数据清洗结果"><a href="#数据清洗结果" class="headerlink" title="数据清洗结果"></a>数据清洗结果</h1><p>上面的MapReduce程序执行成功后，可以看到在HDFS中生成的数据输出目录：<br><img src= "/img/loading.gif" data-src="/images/hadoop3.png"><br>我们可以下载其中一个结果数据文件，并用Notepadd++打开查看其数据信息：<br><img src= "/img/loading.gif" data-src="/images/hadoop4.png"></p>
<h1 id="数据处理："><a href="#数据处理：" class="headerlink" title="数据处理："></a>数据处理：</h1><p>对规整数据进行统计分析<br>经过数据清洗之后，就得到了我们做数据的分析统计所需要的比较规整的数据，下面就可以进行数据的统计分析了，即按照业务需求，统计出某一天中每个省份的PV和UV。  </p>
<p>我们依然是需要编写MapReduce程序，并且将数据保存到HDFS中，其架构跟前面的数据清洗是一样的：<br><img src= "/img/loading.gif" data-src="/images/hadoop5.png"></p>
<h1 id="数据处理思路："><a href="#数据处理思路：" class="headerlink" title="数据处理思路："></a>数据处理思路：</h1><p>如何编写MapReduce程序<br>现在我们已经得到了规整的数据，关于在于如何编写我们的MapReduce程序。  </p>
<p>因为要统计的是每个省对应的pv和uv，pv就是点击量，uv是独立访客量，需要将省相同的数据拉取到一起，拉取到一块的这些数据每一条记录就代表了一次点击（pv + 1），这里面有同一个用户产生的数据（通过mid来唯一地标识是同一个浏览器，用mid进行去重，得到的就是uv）。  </p>
<p>而拉取数据，可以使用Mapper来完成，对数据的统计（pv、uv的计算）则可以通过Reducer来完成，即Mapper的各个参数可以为如下： </p>
<pre><code>Mapper&lt;LongWritable, Text, Text(Province), Text(mid)&gt;</code></pre><p>而Reducer的各个参数可以为如下：</p>
<pre><code>Reducer&lt;Text(Province), Text(mid), Text(Province), Text(pv + uv)&gt;</code></pre><p>数据处理过程：MapReduce程序编写<br>根据前面的分析，来编写我们的MapReduce程序。<br>ProvincePVAndUVMapper</p>
<pre><code>package cn.xpleaf.dataClean.mr.mapper;

import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;

import java.io.IOException;

/**
 * Mapper&lt;LongWritable, Text, Text(Province), Text(mid)&gt;
 * Reducer&lt;Text(Province), Text(mid), Text(Province), Text(pv + uv)&gt;
 */
public class ProvincePVAndUVMapper extends Mapper&lt;LongWritable, Text, Text, Text&gt; {
    @Override
    protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
        String line = value.toString();
        String[] fields = line.split(&quot;\t&quot;);
        if(fields == null || fields.length != 16) {
            return;
        }
        String province = fields[2];
        String mid = fields[4];
        context.write(new Text(province), new Text(mid));
    }
}</code></pre><p>ProvincePVAndUVReducer</p>
<pre><code>package cn.xpleaf.dataClean.mr.reducer;

import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Reducer;

import java.io.IOException;
import java.util.HashSet;
import java.util.Set;

/**
 * 统计该标准化数据，产生结果
 * 省    pv      uv
 * 这里面有同一个用户产生的数|据（通过mid来唯一地标识是同一个浏览器，用mid进行去重，得到的就是uv）
 * Mapper&lt;LongWritable, Text, Text(Province), Text(mid)&gt;
 * Reducer&lt;Text(Province), Text(mid), Text(Province), Text(pv + uv)&gt;
 */
public class ProvincePVAndUVReducer extends Reducer&lt;Text, Text, Text, Text&gt; {

    private Set&lt;String&gt; uvSet = new HashSet&lt;&gt;();

    @Override
    protected void reduce(Text key, Iterable&lt;Text&gt; values, Context context) throws IOException, InterruptedException {
        long pv = 0;
        uvSet.clear();
        for(Text mid : values) {
            pv++;
            uvSet.add(mid.toString());
        }
        long uv = uvSet.size();
        String pvAndUv = pv + &quot;\t&quot; + uv;
        context.write(key, new Text(pvAndUv));
    }
}</code></pre><p>ProvincePVAndUVJob</p>
<pre><code>package cn.xpleaf.dataClean.mr.job;

import cn.xpleaf.dataClean.mr.mapper.ProvincePVAndUVMapper;
import cn.xpleaf.dataClean.mr.reducer.ProvincePVAndUVReducer;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;

/**
 * 统计每个省的pv和uv值
 * 输入：经过clean之后的access日志
 *      appid ip province city mid userid login_type request method request_url http_version status http_referer user_agent browser yyyy-MM-dd HH:mm:ss
 * 统计该标准化数据，产生结果
 * 省    pv      uv
 *
 * 分析：因为要统计的是每个省对应的pv和uv
 *      pv就是点击量，uv是独立访客量
 *      需要将省相同的数据拉取到一起，拉取到一块的这些数据每一条记录就代表了一次点击（pv + 1）
 *      这里面有同一个用户产生的数据（通过mid来唯一地标识是同一个浏览器，用mid进行去重，得到的就是uv）
 *      Mapper&lt;LongWritable, Text, Text(Province), Text(mid)&gt;
 *      Reducer&lt;Text(Province), Text(mid), Text(Province), Text(pv + uv)&gt;
 *
 *  输入参数：
 *  hdfs://ns1/output/data-clean/access hdfs://ns1/output/pv-uv
 */
public class ProvincePVAndUVJob {
    public static void main(String[] args) throws Exception {

        if (args == null || args.length &lt; 2) {
            System.err.println(&quot;Parameter Errors! Usage &lt;inputPath...&gt; &lt;outputPath&gt;&quot;);
            System.exit(-1);
        }

        Path outputPath = new Path(args[args.length - 1]);

        Configuration conf = new Configuration();
        String jobName = ProvincePVAndUVJob.class.getSimpleName();
        Job job = Job.getInstance(conf, jobName);
        job.setJarByClass(ProvincePVAndUVJob.class);

        // 设置mr的输入参数
        for (int i = 0; i &lt; args.length - 1; i++) {
            FileInputFormat.addInputPath(job, new Path(args[i]));
        }
        job.setInputFormatClass(TextInputFormat.class);
        job.setMapperClass(ProvincePVAndUVMapper.class);
        job.setMapOutputKeyClass(Text.class);
        job.setMapOutputValueClass(Text.class);
        // 设置mr的输出参数
        outputPath.getFileSystem(conf).delete(outputPath, true);    // 避免job在运行的时候出现输出目录已经存在的异常
        FileOutputFormat.setOutputPath(job, outputPath);
        job.setOutputFormatClass(TextOutputFormat.class);
        job.setReducerClass(ProvincePVAndUVReducer.class);
        job.setOutputKeyClass(Text.class);
        job.setOutputValueClass(Text.class);
        job.setNumReduceTasks(1);

        job.waitForCompletion(true);
    }
}</code></pre><h1 id="执行MapReduce程序-1"><a href="#执行MapReduce程序-1" class="headerlink" title="执行MapReduce程序"></a>执行MapReduce程序</h1><p>将上面的mr程序打包后上传到我们的Hadoop环境中，这里，对前面预处理之后的数据进行统计分析，执行如下命令：</p>
<pre><code>yarn jar data-extract-clean-analysis-1.0-SNAPSHOT-jar-with-dependencies.jar \
cn.xpleaf.dataClean.mr.job.ProvincePVAndUVJob \
hdfs://ns1/output/data-clean/access \
hdfs://ns1/output/pv-uv</code></pre><p>观察运行结果、可以看到MapReduce Job执行成功！</p>
<pre><code>......
18/04/08 22:22:42 INFO mapreduce.Job: Running job: job_1523133033819_0010
18/04/08 22:22:49 INFO mapreduce.Job: Job job_1523133033819_0010 running in uber mode : false
18/04/08 22:22:49 INFO mapreduce.Job:  map 0% reduce 0%
18/04/08 22:22:55 INFO mapreduce.Job:  map 50% reduce 0%
18/04/08 22:22:57 INFO mapreduce.Job:  map 100% reduce 0%
18/04/08 22:23:03 INFO mapreduce.Job:  map 100% reduce 100%
18/04/08 22:23:03 INFO mapreduce.Job: Job job_1523133033819_0010 completed successfully
18/04/08 22:23:03 INFO mapreduce.Job: Counters: 49
......</code></pre><h1 id="数据处理结果"><a href="#数据处理结果" class="headerlink" title="数据处理结果"></a>数据处理结果</h1><p>上面的MapReduce程序执行成功后，可以看到在HDFS中生成的数据输出目录：<br><img src= "/img/loading.gif" data-src="/images/hadoop6.png"><br>我们可以下载其结果数据文件，并用Notepadd++打开查看其数据信息：<br><img src= "/img/loading.gif" data-src="/images/hadoop7.png"><br>至此，就完成了一个完整的数据采集、清洗、处理的完整离线数据分析案例。</p>
<p>相关的代码留言邮箱我Email你，有兴趣可以参考一下</p>
</div><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined">Alexis (KangJ)</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="https://programmer996.club/2020/06/01/%E5%A4%A7%E6%95%B0%E6%8D%AE%E9%87%87%E9%9B%86%E3%80%81%E6%B8%85%E6%B4%97%E3%80%81%E5%A4%84%E7%90%86%EF%BC%9A%E4%BD%BF%E7%94%A8MapReduce%E8%BF%9B%E8%A1%8C%E7%A6%BB%E7%BA%BF%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90-%E5%A4%A7%E6%95%B0%E6%8D%AE/">https://programmer996.club/2020/06/01/%E5%A4%A7%E6%95%B0%E6%8D%AE%E9%87%87%E9%9B%86%E3%80%81%E6%B8%85%E6%B4%97%E3%80%81%E5%A4%84%E7%90%86%EF%BC%9A%E4%BD%BF%E7%94%A8MapReduce%E8%BF%9B%E8%A1%8C%E7%A6%BB%E7%BA%BF%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90-%E5%A4%A7%E6%95%B0%E6%8D%AE/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" rel="noopener">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Java%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91/">Java大数据开发</a></div><div class="post_share"><div class="social-share" data-image="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" data-sites="wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css"/><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js"></script></div></div><div class="post-reward"><button class="reward-button" type="button"><i class="fas fa-qrcode"></i> Donate<div class="reward-main"><ul class="reward-all"><li class="reward-item"><img class="post-qr-code__img" src="/images/pay.jpg" alt="wechat" onclick="window.open('/images/pay.jpg')"/><div class="post-qr-code__desc">wechat</div></li></ul></div></button></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2020/07/02/Pytorch%E4%B8%93%E6%A0%8F%E5%BC%80%E7%AF%87/"><img class="prev-cover" data-src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" onerror="onerror=null;src='/img/404.jpg'"><div class="pagination-info"><div class="label">Previous Post</div><div class="prev_info">PyTorch专栏开篇</div></div></a></div><div class="next-post pull-right"><a href="/2020/06/01/Spark%E7%94%9F%E6%80%81%E5%9C%88-%E5%A4%A7%E6%95%B0%E6%8D%AE/"><img class="next-cover" data-src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" onerror="onerror=null;src='/img/404.jpg'"><div class="pagination-info"><div class="label">Next Post</div><div class="next_info">Spark生态圈---大数据</div></div></a></div></nav><div class="relatedPosts"><div class="relatedPosts_headline"><i class="fas fa-thumbs-up fa-fw"></i><span> Related Articles</span></div><div class="relatedPosts_list"><div class="relatedPosts_item"><a href="/2020/06/01/Spark生态圈-大数据/" title="Spark生态圈---大数据"><img class="relatedPosts_cover" data-src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="far fa-calendar-alt fa-fw"></i> 2020-06-01</div><div class="relatedPosts_title">Spark生态圈---大数据</div></div></a></div></div></div></article></main><footer id="footer" style="background-image: url(https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg)" data-type="photo"><div id="footer-wrap"><div class="copyright">&copy;2018 - 2020 By Alexis (KangJ)</div><div class="framework-info"><span>Driven </span><a href="https://hexo.io" target="_blank" rel="noopener"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme </span><a href="https://github.com/jerryc127/hexo-theme-butterfly" target="_blank" rel="noopener"><span>Butterfly</span></a></div><div class="icp"><a href="http://www.beian.miit.gov.cn/state/outPortal/loginPortal.action" target="_blank" rel="noopener"><img class="icp-icon" src="/img/icp.png"/><span>晋ICP备19013202号</span></a></div></div></footer></div><section id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="font_plus" type="button" title="Increase Font Size"><i class="fas fa-plus"></i></button><button id="font_minus" type="button" title="Decrease Font Size"><i class="fas fa-minus"></i></button><button id="translateLink" type="button" title="Switch Between Traditional Chinese And Simplified Chinese">繁</button><button id="darkmode" type="button" title="Switch Between Light And Dark Mode"><i class="fas fa-adjust"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="Setting"><i class="fas fa-cog"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><button id="chat_btn" type="button" title="rightside.chat_btn"><i class="fas fa-sms"></i></button><button id="go-up" type="button" title="Back To Top"><i class="fas fa-arrow-up"></i></button></div></section><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.js"></script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page/instantpage.min.js" type="module" defer></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js" async></script><script>var endLoading = function () {
  document.body.style.overflow = 'auto';
  document.getElementById('loading-box').classList.add("loaded")
}
window.addEventListener('load',endLoading)</script><script>(function(d, w, c) {
    w.ChatraID = 'KNGoAf4sGNJs6n5zG';
    var s = d.createElement('script');
    w[c] = w[c] || function() {
        (w[c].q = w[c].q || []).push(arguments);
    };
    s.async = true;
    s.src = 'https://call.chatra.io/chatra.js';
    if (d.head) d.head.appendChild(s);
})(document, window, 'Chatra');

if (true) {
  var chatBtn = document.getElementById("chat_btn")
    chatBtn.addEventListener("click", function(){
      Chatra('openChat')
    });
} else {
  if (true) {
    function chatBtnHide () {
      Chatra('hide')
    }
    function chatBtnShow () {
      Chatra('show')
    }
  }
}</script></body></html>