<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>Pytorch专栏（二） | 艾利克斯工作室</title><meta name="description" content="前言本篇文章讲解了PyTorch专栏的第二章中的PyTorch 自动微分和PyTorch 神经网络，查看第二章中的其他内容，请点击查看专栏目录里相应的内容，希望对大家有所帮助。查看关于本专栏的介绍：PyTorch专栏开篇。   PyTorch之60min入门PyTorch 自动微分autograd 包是 PyTorch 中所有神经网络的核心。首先让我们简要地介绍它，然后我们将会去训练我们的第一个神"><meta name="keywords" content="Python深度学习"><meta name="author" content="Alexis (KangJ)"><meta name="copyright" content="Alexis (KangJ)"><meta name="format-detection" content="telephone=no"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://programmer996.club/2020/07/06/Pytorch%E4%B8%93%E6%A0%8F%EF%BC%88%E4%BA%8C%EF%BC%89/"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//fonts.googleapis.com" crossorigin="crossorigin"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><meta property="og:type" content="article"><meta property="og:title" content="Pytorch专栏（二）"><meta property="og:url" content="https://programmer996.club/2020/07/06/Pytorch%E4%B8%93%E6%A0%8F%EF%BC%88%E4%BA%8C%EF%BC%89/"><meta property="og:site_name" content="艾利克斯工作室"><meta property="og:description" content="前言本篇文章讲解了PyTorch专栏的第二章中的PyTorch 自动微分和PyTorch 神经网络，查看第二章中的其他内容，请点击查看专栏目录里相应的内容，希望对大家有所帮助。查看关于本专栏的介绍：PyTorch专栏开篇。   PyTorch之60min入门PyTorch 自动微分autograd 包是 PyTorch 中所有神经网络的核心。首先让我们简要地介绍它，然后我们将会去训练我们的第一个神"><meta property="og:image" content="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg"><meta property="article:published_time" content="2020-07-06T01:32:11.000Z"><meta property="article:modified_time" content="2020-07-10T00:55:21.885Z"><meta name="twitter:card" content="summary"><script>var activateDarkMode = function () {
  document.documentElement.setAttribute('data-theme', 'dark')
  if (document.querySelector('meta[name="theme-color"]') !== null) {
    document.querySelector('meta[name="theme-color"]').setAttribute('content', '#000')
  }
}
var activateLightMode = function () {
  document.documentElement.setAttribute('data-theme', 'light')
  if (document.querySelector('meta[name="theme-color"]') !== null) {
    document.querySelector('meta[name="theme-color"]').setAttribute('content', '#fff')
  }
}

var getCookies = function (name) {
  const value = `; ${document.cookie}`
  const parts = value.split(`; ${name}=`)
  if (parts.length === 2) return parts.pop().split(';').shift()
}

var autoChangeMode = 'false'
var t = getCookies('theme')
if (autoChangeMode === '1') {
  var isDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches
  var isLightMode = window.matchMedia('(prefers-color-scheme: light)').matches
  var isNotSpecified = window.matchMedia('(prefers-color-scheme: no-preference)').matches
  var hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

  if (t === undefined) {
    if (isLightMode) activateLightMode()
    else if (isDarkMode) activateDarkMode()
    else if (isNotSpecified || hasNoSupport) {
      console.log('You specified no preference for a color scheme or your browser does not support it. I Schedule dark mode during night time.')
      var now = new Date()
      var hour = now.getHours()
      var isNight = hour <= 6 || hour >= 18
      isNight ? activateDarkMode() : activateLightMode()
    }
    window.matchMedia('(prefers-color-scheme: dark)').addListener(function (e) {
      if (Cookies.get('theme') === undefined) {
        e.matches ? activateDarkMode() : activateLightMode()
      }
    })
  } else if (t === 'light') activateLightMode()
  else activateDarkMode()
} else if (autoChangeMode === '2') {
  now = new Date()
  hour = now.getHours()
  isNight = hour <= 6 || hour >= 18
  if (t === undefined) isNight ? activateDarkMode() : activateLightMode()
  else if (t === 'light') activateLightMode()
  else activateDarkMode()
} else {
  if (t === 'dark') activateDarkMode()
  else if (t === 'light') activateLightMode()
}</script><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.css"><link rel="prev" title="PyTorch专栏（三）" href="https://programmer996.club/2020/07/10/PyTorch%E4%B8%93%E6%A0%8F%EF%BC%88%E4%B8%89%EF%BC%89/"><link rel="next" title="Pytorch专栏（一）" href="https://programmer996.club/2020/07/03/Pytorch%E4%B8%93%E6%A0%8F%EF%BC%88%E4%B8%80%EF%BC%89/"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web&amp;display=swap"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  noticeOutdate: undefined,
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  bookmark: {
    message_prev: 'Press',
    message_next: 'to bookmark this page'
  },
  runtime_unit: 'days',
  runtime: false,
  copyright: undefined,
  ClickShowText: undefined,
  medium_zoom: false,
  fancybox: true,
  Snackbar: {"bookmark":{"message_prev":"Press","message_next":"to bookmark this page"},"chs_to_cht":"Traditional Chinese Activated Manually","cht_to_chs":"Simplified Chinese Activated Manually","day_to_night":"Dark Mode Activated Manually","night_to_day":"Light Mode Activated Manually","bgLight":"#49b1f5","bgDark":"#121212","position":"bottom-left"},
  justifiedGallery: {
    js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
    css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
  },
  baiduPush: false,
  highlightCopy: true,
  highlightLang: true,
  isPhotoFigcaption: false,
  islazyload: true,
  isanchor: false    
}</script><script>var GLOBAL_CONFIG_SITE = { 
  isPost: true,
  isHome: false,
  isHighlightShrink: true,
  isSidebar: true,
  postUpdate: '2020-07-10 08:55:21'
}</script><noscript><style>
#nav {
  opacity: 1
}
.justified-gallery img{
  opacity: 1
}
</style></noscript><meta name="generator" content="Hexo 4.2.1"><link rel="alternate" href="/atom.xml" title="艾利克斯工作室" type="application/atom+xml">
</head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">Loading...</div></div></div><div id="mobile-sidebar"><div id="menu_mask"></div><div id="mobile-sidebar-menus"><div class="mobile_author_icon"><img class="avatar-img" src="/img/avatar.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="mobile_post_data"><div class="mobile_data_item is-center"><div class="mobile_data_link"><a href="/archives/"><div class="headline">Articles</div><div class="length_num">44</div></a></div></div><div class="mobile_data_item is-center">      <div class="mobile_data_link"><a href="/tags/"><div class="headline">Tags</div><div class="length_num">9</div></a></div></div><div class="mobile_data_item is-center">     <div class="mobile_data_link"><a href="/categories/"><div class="headline">Categories</div><div class="length_num">16</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page"><i class="fa-fw fas fa-list"></i><span> List</span><i class="fas fa-chevron-down menus-expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/welfare/"><i class="fa-fw fas fa-gift"></i><span> Welfare</span></a></li><li><a class="site-page" href="/Case/"><i class="fa-fw fas fa-briefcase"></i><span> Case</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div><div class="menus_item"><a class="site-page" href="/Cooperation/"><i class="fa-fw fas fa-briefcase"></i><span> Cooperation</span></a></div><div class="menus_item"><a class="site-page" href="/handshake/"><i class="fa-fw fas fa-eye"></i><span> handshake</span></a></div></div></div></div><i class="fas fa-arrow-right on" id="toggle-sidebar"></i><div id="sidebar"><div class="sidebar-toc"><div class="sidebar-toc__title">Catalog</div><div class="sidebar-toc__progress"><span class="progress-notice">You've read</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar">     </div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#前言"><span class="toc-number">1.</span> <span class="toc-text">前言</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#PyTorch之60min入门"><span class="toc-number">2.</span> <span class="toc-text">PyTorch之60min入门</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#PyTorch-自动微分"><span class="toc-number">3.</span> <span class="toc-text">PyTorch 自动微分</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#1、TENSOR"><span class="toc-number">4.</span> <span class="toc-text">1、TENSOR</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#PyTorch神经网络"><span class="toc-number">5.</span> <span class="toc-text">PyTorch神经网络</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#学习交流"><span class="toc-number">6.</span> <span class="toc-text">学习交流</span></a></li></ol></div></div></div><div class="code-close" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg)"><nav id="nav"><span class="pull-left" id="blog_name"><a class="blog_title" id="site-name" href="/">艾利克斯工作室</a></span><span class="pull-right menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page"><i class="fa-fw fas fa-list"></i><span> List</span><i class="fas fa-chevron-down menus-expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/welfare/"><i class="fa-fw fas fa-gift"></i><span> Welfare</span></a></li><li><a class="site-page" href="/Case/"><i class="fa-fw fas fa-briefcase"></i><span> Case</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div><div class="menus_item"><a class="site-page" href="/Cooperation/"><i class="fa-fw fas fa-briefcase"></i><span> Cooperation</span></a></div><div class="menus_item"><a class="site-page" href="/handshake/"><i class="fa-fw fas fa-eye"></i><span> handshake</span></a></div></div><span class="toggle-menu close"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></span></span></nav><div id="post-info"><div id="post-title"><div class="posttitle">Pytorch专栏（二）</div></div><div id="post-meta"><div class="meta-firstline"><time class="post-meta__date"><span class="post-meta__date-created" title="Created 2020-07-06 09:32:11"><i class="far fa-calendar-alt fa-fw"></i> Created 2020-07-06</span><span class="post-meta__separator">|</span><span class="post-meta__date-updated" title="Updated 2020-07-10 08:55:21"><i class="fas fa-history fa-fw"></i> Updated 2020-07-10</span></time><span class="post-meta__categories"><span class="post-meta__separator">|</span><i class="fas fa-inbox fa-fw post-meta__icon"></i><a class="post-meta__categories" href="/categories/PyTorch%E4%B8%93%E6%A0%8F/">PyTorch专栏</a></span></div><div class="meta-secondline"> <span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta__icon"></i><span>Word count:</span><span class="word-count">2.6k</span><span class="post-meta__separator">|</span><i class="far fa-clock fa-fw post-meta__icon"></i><span>Reading time: 10 min</span></span></div><div class="meta-thirdline"><span class="post-meta-pv-cv"><span class="post-meta__separator">|</span><i class="far fa-eye fa-fw post-meta__icon"></i><span>Post View:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout_post" id="content-inner"><article id="post"><div class="post-content" id="article-container"><h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>本篇文章讲解了PyTorch专栏的第二章中的PyTorch 自动微分和PyTorch 神经网络，查看第二章中的其他内容，请点击查看专栏目录里相应的内容，希望对大家有所帮助。查看关于本专栏的介绍：PyTorch专栏开篇。<a id="more"></a>  </p>
<h1 id="PyTorch之60min入门"><a href="#PyTorch之60min入门" class="headerlink" title="PyTorch之60min入门"></a>PyTorch之60min入门</h1><h1 id="PyTorch-自动微分"><a href="#PyTorch-自动微分" class="headerlink" title="PyTorch 自动微分"></a>PyTorch 自动微分</h1><p>autograd 包是 PyTorch 中所有神经网络的核心。首先让我们简要地介绍它，然后我们将会去训练我们的第一个神经网络。该 autograd 软件包为 Tensors 上的所有操作提供自动微分。它是一个由运行定义的框架，这意味着以代码运行方式定义你的后向传播，并且每次迭代都可以不同。我们从 tensor 和 gradients 来举一些例子。</p>
<h1 id="1、TENSOR"><a href="#1、TENSOR" class="headerlink" title="1、TENSOR"></a>1、TENSOR</h1><p>torch.Tensor 是包的核心类。如果将其属性 .requires_grad 设置为 True，则会开始跟踪针对 tensor 的所有操作。完成计算后，您可以调用 .backward() 来自动计算所有梯度。该张量的梯度将累积到 .grad 属性中。</p>
<p>要停止 tensor 历史记录的跟踪，您可以调用 .detach()，它将其与计算历史记录分离，并防止将来的计算被跟踪。</p>
<p>要停止跟踪历史记录（和使用内存），您还可以将代码块使用 with torch.no_grad(): 包装起来。在评估模型时，这是特别有用，因为模型在训练阶段具有 requires_grad = True 的可训练参数有利于调参，但在评估阶段我们不需要梯度。</p>
<p>还有一个类对于 autograd 实现非常重要那就是 Function。Tensor 和 Function 互相连接并构建一个非循环图，它保存整个完整的计算过程的历史信息。每个张量都有一个 .grad_fn 属性保存着创建了张量的 Function 的引用，（如果用户自己创建张量，则g rad_fn 是 None ）。</p>
<p>如果你想计算导数，你可以调用 Tensor.backward()。如果 Tensor 是标量（即它包含一个元素数据），则不需要指定任何参数backward()，但是如果它有更多元素，则需要指定一个gradient 参数来指定张量的形状。</p>
<pre><code>import torch</code></pre><p>创建一个张量，设置 requires_grad=True 来跟踪与它相关的计算</p>
<pre><code>x = torch.ones(2, 2, requires_grad=True)
print(x)</code></pre><p>输出：</p>
<pre><code>tensor([[1., 1.],
        [1., 1.]], requires_grad=True)</code></pre><p>针对张量做一个操作</p>
<pre><code>y = x + 2
print(y)</code></pre><p>输出：</p>
<pre><code>tensor([[3., 3.],
        [3., 3.]], grad_fn=&lt;AddBackward0&gt;)</code></pre><p>y 作为操作的结果被创建，所以它有 grad_fn</p>
<pre><code>print(y.grad_fn)</code></pre><p>输出：</p>
<pre><code>&lt;AddBackward0 object at 0x7fe1db427470&gt;</code></pre><p>针对 y 做更多的操作：</p>
<pre><code>z = y * y * 3
out = z.mean()

print(z, out)</code></pre><p>输出：</p>
<pre><code>tensor([[27., 27.],
        [27., 27.]], grad_fn=&lt;MulBackward0&gt;) tensor(27., grad_fn=&lt;MeanBackward0&gt;)</code></pre><p>.requires_grad_(…) 会改变张量的requires_gra 标记。输入的标记默认为False ，如果没有提供相应的参数。</p>
<pre><code>a = torch.randn(2, 2)
a = ((a * 3) / (a - 1))
print(a.requires_grad)
a.requires_grad_(True)
print(a.requires_grad)
b = (a * a).sum()
print(b.grad_fn)</code></pre><p>输出：</p>
<pre><code>False
True
&lt;SumBackward0 object at 0x7fe1db427dd8&gt;</code></pre><p>梯度：</p>
<p>我们现在后向传播，因为输出包含了一个标量，out.backward() 等同于out.backward(torch.tensor(1.))。</p>
<pre><code>out.backward()</code></pre><p>打印梯度  d(out)/dx</p>
<pre><code>print(x.grad)</code></pre><p>输出：</p>
<pre><code>tensor([[4.5000, 4.5000],
        [4.5000, 4.5000]])</code></pre><p>原理解释：<br><img src= "/img/loading.gif" data-src="/images/p13.jpg"></p>
<p>现在让我们看一个雅可比向量积的例子：</p>
<pre><code>x = torch.randn(3, requires_grad=True)

y = x * 2
while y.data.norm() &lt; 1000:
    y = y * 2

print(y)</code></pre><p>输出：</p>
<pre><code>tensor([ -444.6791,   762.9810, -1690.0941], grad_fn=&lt;MulBackward0&gt;)</code></pre><p>现在在这种情况下，y 不再是一个标量。torch.autograd 不能够直接计算整个雅可比，但是如果我们只想要雅可比向量积，只需要简单的传递向量给 backward 作为参数。</p>
<pre><code>v = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float)
y.backward(v)

print(x.grad)</code></pre><p>输出：</p>
<pre><code>tensor([1.0240e+02, 1.0240e+03, 1.0240e-01])</code></pre><p>你可以通过将代码包裹在 with torch.no_grad()，来停止对从跟踪历史中 的 .requires_grad=True 的张量自动求导。</p>
<pre><code>print(x.requires_grad)
print((x ** 2).requires_grad)

with torch.no_grad():
    print((x ** 2).requires_grad)</code></pre><p>输出：</p>
<pre><code>True
True
False</code></pre><p>下载 Python 源代码：</p>
<pre><code>autograd_tutorial.py</code></pre><p>下载 Jupyter 源代码：</p>
<pre><code>autograd_tutorial.ipynb</code></pre><h1 id="PyTorch神经网络"><a href="#PyTorch神经网络" class="headerlink" title="PyTorch神经网络"></a>PyTorch神经网络</h1><p>神经网络可以通过 torch.nn 包来构建。</p>
<p>现在对于自动梯度(autograd)有一些了解，神经网络是基于自动梯度 (autograd)来定义一些模型。一个 nn.Module 包括层和一个方法 forward(input) 它会返回输出(output)。</p>
<p>例如，看一下数字图片识别的网络:<br><img src= "/img/loading.gif" data-src="/images/p14.jpg"><br>这是一个简单的前馈神经网络，它接收输入，让输入一个接着一个的通过一些层，最后给出输出。</p>
<p>一个典型的神经网络训练过程包括以下几点：</p>
<p>1.定义一个包含可训练参数的神经网络</p>
<p>2.迭代整个输入</p>
<p>3.通过神经网络处理输入</p>
<p>4.计算损失(loss)</p>
<p>5.反向传播梯度到神经网络的参数</p>
<p>6.更新网络的参数，典型的用一个简单的更新方法：weight = weight - learning_rate *gradient</p>
<p>定义神经网络</p>
<pre><code>import torch
import torch.nn as nn
import torch.nn.functional as F


class Net(nn.Module):

    def __init__(self):
        super(Net, self).__init__()
        # 1 input image channel, 6 output channels, 5x5 square convolution
        # kernel
        self.conv1 = nn.Conv2d(1, 6, 5)
        self.conv2 = nn.Conv2d(6, 16, 5)
        # an affine operation: y = Wx + b
        self.fc1 = nn.Linear(16 * 5 * 5, 120)
        self.fc2 = nn.Linear(120, 84)
        self.fc3 = nn.Linear(84, 10)

    def forward(self, x):
        # Max pooling over a (2, 2) window
        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))
        # If the size is a square you can only specify a single number
        x = F.max_pool2d(F.relu(self.conv2(x)), 2)
        x = x.view(-1, self.num_flat_features(x))
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x

    def num_flat_features(self, x):
        size = x.size()[1:]  # all dimensions except the batch dimension
        num_features = 1
        for s in size:
            num_features *= s
        return num_features


net = Net()
print(net)</code></pre><p>输出:</p>
<pre><code>Net(
  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))
  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=400, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=84, bias=True)
  (fc3): Linear(in_features=84, out_features=10, bias=True)
)</code></pre><p>你刚定义了一个前馈函数，然后反向传播函数被自动通过 autograd 定义了。你可以使用任何张量操作在前馈函数上。</p>
<p>一个模型可训练的参数可以通过调用 net.parameters() 返回：</p>
<pre><code>params = list(net.parameters())
print(len(params))
print(params[0].size())  # conv1&#39;s .weight</code></pre><p>输出：</p>
<pre><code>10
torch.Size([6, 1, 5, 5])</code></pre><p>让我们尝试随机生成一个 32x32 的输入。注意：期望的输入维度是 32x32 。为了使用这个网络在 MNIST 数据及上，你需要把数据集中的图片维度修改为 32x32。</p>
<pre><code>input = torch.randn(1, 1, 32, 32)
out = net(input)
print(out)</code></pre><p>输出：</p>
<pre><code>tensor([[-0.0233,  0.0159, -0.0249,  0.1413,  0.0663,  0.0297, -0.0940, -0.0135,
          0.1003, -0.0559]], grad_fn=&lt;AddmmBackward&gt;)</code></pre><p>把所有参数梯度缓存器置零，用随机的梯度来反向传播</p>
<pre><code>net.zero_grad()
out.backward(torch.randn(1, 10))</code></pre><p>在继续之前，让我们复习一下所有见过的类。</p>
<pre><code>torch.Tensor - A multi-dimensional array with support for autograd operations like backward(). Also holds the gradient w.r.t. the tensor.
nn.Module - Neural network module. Convenient way of encapsulating parameters, with helpers for moving them to GPU, exporting, loading, etc.
nn.Parameter - A kind of Tensor, that is automatically registered as a parameter when assigned as an attribute to a Module.
autograd.Function - Implements forward and backward definitions of an autograd operation. Every Tensor operation, creates at least a single Function node, that connects to functions that created a Tensor and encodes its history.</code></pre><p>在此，我们完成了：</p>
<p>1.定义一个神经网络</p>
<p>2.处理输入以及调用反向传播</p>
<p>还剩下：</p>
<p>1.计算损失值</p>
<p>2.更新网络中的权重</p>
<p>损失函数</p>
<p>一个损失函数需要一对输入：模型输出和目标，然后计算一个值来评估输出距离目标有多远。</p>
<p>有一些不同的损失函数在 nn 包中。一个简单的损失函数就是 nn.MSELoss ，这计算了均方误差。</p>
<p>例如：</p>
<pre><code>output = net(input)
target = torch.randn(10)  # a dummy target, for example
target = target.view(1, -1)  # make it the same shape as output
criterion = nn.MSELoss()

loss = criterion(output, target)
print(loss)</code></pre><p>输出：</p>
<pre><code>tensor(1.3389, grad_fn=&lt;MseLossBackward&gt;)</code></pre><p>现在，如果你跟随损失到反向传播路径，可以使用它的 .grad_fn 属性，你将会看到一个这样的计算图：</p>
<pre><code>input -&gt; conv2d -&gt; relu -&gt; maxpool2d -&gt; conv2d -&gt; relu -&gt; maxpool2d
      -&gt; view -&gt; linear -&gt; relu -&gt; linear -&gt; relu -&gt; linear
      -&gt; MSELoss
      -&gt; loss</code></pre><p>所以，当我们调用 loss.backward()，整个图都会微分，而且所有的在图中的requires_grad=True 的张量将会让他们的 grad 张量累计梯度。</p>
<p>为了演示，我们将跟随以下步骤来反向传播。</p>
<pre><code>print(loss.grad_fn)  # MSELoss
print(loss.grad_fn.next_functions[0][0])  # Linear
print(loss.grad_fn.next_functions[0][0].next_functions[0][0])  # ReLU</code></pre><p>输出：</p>
<pre><code>&lt;MseLossBackward object at 0x7fab77615278&gt;
&lt;AddmmBackward object at 0x7fab77615940&gt;
&lt;AccumulateGrad object at 0x7fab77615940&gt;</code></pre><p>反向传播</p>
<p>为了实现反向传播损失，我们所有需要做的事情仅仅是使用 loss.backward()。你需要清空现存的梯度，要不然帝都将会和现存的梯度累计到一起。</p>
<p>现在我们调用 loss.backward() ，然后看一下 con1 的偏置项在反向传播之前和之后的变化。</p>
<pre><code>net.zero_grad()     # zeroes the gradient buffers of all parameters

print(&#39;conv1.bias.grad before backward&#39;)
print(net.conv1.bias.grad)

loss.backward()

print(&#39;conv1.bias.grad after backward&#39;)
print(net.conv1.bias.grad)</code></pre><p>输出：</p>
<pre><code>conv1.bias.grad before backward
tensor([0., 0., 0., 0., 0., 0.])
conv1.bias.grad after backward
tensor([-0.0054,  0.0011,  0.0012,  0.0148, -0.0186,  0.0087])</code></pre><p>现在我们看到了，如何使用损失函数。</p>
<p>唯一剩下的事情就是更新神经网络的参数。</p>
<p>更新神经网络参数：</p>
<p>最简单的更新规则就是随机梯度下降。</p>
<pre><code>weight = weight - learning_rate * gradient</code></pre><p>我们可以使用 python 来实现这个规则：</p>
<pre><code>learning_rate = 0.01
for f in net.parameters():
    f.data.sub_(f.grad.data * learning_rate)</code></pre><p>尽管如此，如果你是用神经网络，你想使用不同的更新规则，类似于 SGD, Nesterov-SGD, Adam, RMSProp, 等。为了让这可行，我们建立了一个小包：torch.optim 实现了所有的方法。使用它非常的简单。</p>
<pre><code>import torch.optim as optim

# create your optimizer
optimizer = optim.SGD(net.parameters(), lr=0.01)

# in your training loop:
optimizer.zero_grad()   # zero the gradient buffers
output = net(input)
loss = criterion(output, target)
loss.backward()
optimizer.step()    # Does the update</code></pre><p>下载 Python 源代码：</p>
<pre><code>neural_networks_tutorial.py</code></pre><p>下载 Jupyter 源代码：</p>
<pre><code>neural_networks_tutorial.ipynb</code></pre><h1 id="学习交流"><a href="#学习交流" class="headerlink" title="学习交流"></a>学习交流</h1><p>为了方便大家更好地与作者进行沟通交流，为此针对这个专栏成立了QQ读者交流群，大家想近距离与我沟通，都可以来加入。<br>扫描二维码进群可获得 自动微分 章节的Python 源代码和Jupyter 源代码链接<br>扫描二维码进群可获得 神经网络 章节的Python 源代码和Jupyter 源代码链接<br>加入方式：扫描下方QQ群二维码，即可加入交流群&lt;群满可在右下方在线与我沟通&gt;。</p>
<p align="left">
    <img src= "/img/loading.gif" data-src="/images/qun.jpg" alt="Sample" width="300" height="350">
    <!-- <img src= "/img/loading.gif" data-src="/images/TIM.jpg" alt="Sample"  width="300" height="250"> -->
</p></div><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined">Alexis (KangJ)</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="https://programmer996.club/2020/07/06/Pytorch%E4%B8%93%E6%A0%8F%EF%BC%88%E4%BA%8C%EF%BC%89/">https://programmer996.club/2020/07/06/Pytorch%E4%B8%93%E6%A0%8F%EF%BC%88%E4%BA%8C%EF%BC%89/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" rel="noopener">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Python%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">Python深度学习</a></div><div class="post_share"><div class="social-share" data-image="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" data-sites="wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css"/><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js"></script></div></div><div class="post-reward"><button class="reward-button" type="button"><i class="fas fa-qrcode"></i> Donate<div class="reward-main"><ul class="reward-all"><li class="reward-item"><img class="post-qr-code__img" src="/images/pay.jpg" alt="wechat" onclick="window.open('/images/pay.jpg')"/><div class="post-qr-code__desc">wechat</div></li></ul></div></button></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2020/07/10/PyTorch%E4%B8%93%E6%A0%8F%EF%BC%88%E4%B8%89%EF%BC%89/"><img class="prev-cover" data-src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" onerror="onerror=null;src='/img/404.jpg'"><div class="pagination-info"><div class="label">Previous Post</div><div class="prev_info">PyTorch专栏（三）</div></div></a></div><div class="next-post pull-right"><a href="/2020/07/03/Pytorch%E4%B8%93%E6%A0%8F%EF%BC%88%E4%B8%80%EF%BC%89/"><img class="next-cover" data-src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" onerror="onerror=null;src='/img/404.jpg'"><div class="pagination-info"><div class="label">Next Post</div><div class="next_info">Pytorch专栏（一）</div></div></a></div></nav><div class="relatedPosts"><div class="relatedPosts_headline"><i class="fas fa-thumbs-up fa-fw"></i><span> Related Articles</span></div><div class="relatedPosts_list"><div class="relatedPosts_item"><a href="/2020/07/02/Pytorch专栏开篇/" title="PyTorch专栏开篇"><img class="relatedPosts_cover" data-src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="far fa-calendar-alt fa-fw"></i> 2020-07-02</div><div class="relatedPosts_title">PyTorch专栏开篇</div></div></a></div><div class="relatedPosts_item"><a href="/2020/07/03/Pytorch专栏（一）/" title="Pytorch专栏（一）"><img class="relatedPosts_cover" data-src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="far fa-calendar-alt fa-fw"></i> 2020-07-03</div><div class="relatedPosts_title">Pytorch专栏（一）</div></div></a></div><div class="relatedPosts_item"><a href="/2020/07/10/PyTorch专栏（三）/" title="PyTorch专栏（三）"><img class="relatedPosts_cover" data-src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="far fa-calendar-alt fa-fw"></i> 2020-07-10</div><div class="relatedPosts_title">PyTorch专栏（三）</div></div></a></div></div></div></article></main><footer id="footer" style="background-image: url(https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg)" data-type="photo"><div id="footer-wrap"><div class="copyright">&copy;2018 - 2020 By Alexis (KangJ)</div><div class="framework-info"><span>Driven </span><a href="https://hexo.io" target="_blank" rel="noopener"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme </span><a href="https://github.com/jerryc127/hexo-theme-butterfly" target="_blank" rel="noopener"><span>Butterfly</span></a></div><div class="icp"><a href="http://www.beian.miit.gov.cn/state/outPortal/loginPortal.action" target="_blank" rel="noopener"><img class="icp-icon" src="/img/icp.png"/><span>晋ICP备19013202号</span></a></div></div></footer></div><section id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="font_plus" type="button" title="Increase Font Size"><i class="fas fa-plus"></i></button><button id="font_minus" type="button" title="Decrease Font Size"><i class="fas fa-minus"></i></button><button id="translateLink" type="button" title="Switch Between Traditional Chinese And Simplified Chinese">繁</button><button id="darkmode" type="button" title="Switch Between Light And Dark Mode"><i class="fas fa-adjust"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="Setting"><i class="fas fa-cog"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><button id="chat_btn" type="button" title="rightside.chat_btn"><i class="fas fa-sms"></i></button><button id="go-up" type="button" title="Back To Top"><i class="fas fa-arrow-up"></i></button></div></section><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.js"></script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page/instantpage.min.js" type="module" defer></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js" async></script><script>var endLoading = function () {
  document.body.style.overflow = 'auto';
  document.getElementById('loading-box').classList.add("loaded")
}
window.addEventListener('load',endLoading)</script><script>(function(d, w, c) {
    w.ChatraID = 'KNGoAf4sGNJs6n5zG';
    var s = d.createElement('script');
    w[c] = w[c] || function() {
        (w[c].q = w[c].q || []).push(arguments);
    };
    s.async = true;
    s.src = 'https://call.chatra.io/chatra.js';
    if (d.head) d.head.appendChild(s);
})(document, window, 'Chatra');

if (true) {
  var chatBtn = document.getElementById("chat_btn")
    chatBtn.addEventListener("click", function(){
      Chatra('openChat')
    });
} else {
  if (true) {
    function chatBtnHide () {
      Chatra('hide')
    }
    function chatBtnShow () {
      Chatra('show')
    }
  }
}</script></body></html>